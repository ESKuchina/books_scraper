"""
books_scraper.scraper

–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö
—Å —Å–∞–π—Ç–∞ https://books.toscrape.com.

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–¥–Ω–æ–π –∫–Ω–∏–≥–µ (get_book_data);
- –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ (scrape_books);
- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è;
- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ artifacts/books_data.txt;
- –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (19:00).

–ê–≤—Ç–æ—Ä: Ekaterina Kuchina, –ú–§–¢–ò, 2025 –≥.
"""

import os
import time
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import schedule
from bs4 import BeautifulSoup

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_URL = "https://books.toscrape.com/"
OUTPUT_PATH = os.path.join(BASE_DIR, "artifacts", "books_data.txt")


def get_book_data(book_url: str) -> dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ —Å –µ—ë —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Parameters
    ----------
    book_url : str
        –ü–æ–ª–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏.

    Returns
    -------
    dict
        –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
        title, price, availability, rating, description,
        upc, product_type, price_excl_tax, price_incl_tax,
        tax, availability_count, number_of_reviews.
    """
    with requests.get(book_url, timeout=15) as response:
        response.encoding = (
            "utf-8"
            if "utf" in response.apparent_encoding.lower()
            else "ISO-8859-1"
        )
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")

    title = soup.find("div", class_="product_main").h1.get_text(strip=True)
    price = soup.find("p", class_="price_color").get_text(strip=True)
    availability = soup.find(
        "p", class_="instock availability"
    ).get_text(strip=True)

    rating = soup.find("p", class_="star-rating")["class"][1]

    description_tag = soup.find("div", id="product_description")
    description = (
        description_tag.find_next_sibling("p").get_text(strip=True)
        if description_tag
        else ""
    )

    info_table = soup.find("table", class_="table table-striped")
    info = {
        row.th.get_text(strip=True): row.td.get_text(strip=True)
        for row in info_table.find_all("tr")
    }

    return {
        "title": title,
        "price": price,
        "availability": availability,
        "rating": rating,
        "description": description,
        "upc": info.get("UPC", ""),
        "product_type": info.get("Product Type", ""),
        "price_excl_tax": info.get("Price (excl. tax)", ""),
        "price_incl_tax": info.get("Price (incl. tax)", ""),
        "tax": info.get("Tax", ""),
        "availability_count": info.get("Availability", ""),
        "number_of_reviews": info.get("Number of reviews", ""),
    }


def _fetch_page(
    session: requests.Session,
    page_url: str,
    timeout: int
) -> list[str]:

    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–Ω–∏–≥–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    with session.get(page_url, timeout=timeout) as response:
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")
        return [urljoin(page_url, a["href"]) for a in soup.select("h3 > a")]


def scrape_books(  # pylint: disable=too-many-branches, too-many-locals
    is_save: bool = True,
    use_threads: bool = False,
    max_pages: int | None = None,
    output_path: str | None = None,
    per_request_timeout: int = 30,
) -> list[dict]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–Ω–∏–≥–∞—Ö —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ Books to Scrape.
    """
    all_books = []
    page = 1
    session = requests.Session()

    while True:
        page_url = (
            f"{BASE_URL}catalogue/page-{page}.html"
            if page > 1
            else f"{BASE_URL}index.html"
        )
        try:
            book_links = _fetch_page(session, page_url, per_request_timeout)
        except requests.RequestException as exc:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {page_url}: {exc}")
            break

        if not book_links:
            break

        print(f"üìÑ Page {page} ‚Üí {len(book_links)} books")

        if use_threads:
            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = [
                    executor.submit(get_book_data, link)
                    for link in book_links
                ]

                for f in as_completed(futures):
                    try:
                        all_books.append(f.result())
                    except requests.RequestException as exc:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –≤ –ø–æ—Ç–æ–∫–µ: {exc}")
                    except (RuntimeError, ExceptionGroup) as exc:
                        print(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ: {exc}")
        else:
            for link in book_links:
                try:
                    all_books.append(get_book_data(link))
                    time.sleep(0.05)
                except requests.RequestException as exc:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {exc}")

        if max_pages and page >= max_pages:
            break

        next_button = _fetch_page(session, page_url, per_request_timeout)
        if not next_button:
            break
        page += 1

    if is_save:
        save_path = output_path or OUTPUT_PATH
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as file:
            for book in all_books:
                file.write(str(book) + "\n")
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_books)} –∫–Ω–∏–≥ –≤ {save_path}")

    return all_books


def job() -> None:
    """–ï–¥–∏–Ω–∏—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–Ω–∏–≥."""
    print("\nüïñ –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    scrape_books(is_save=True, use_threads=True)
    print("‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


def run_scheduler() -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –≤ 19:00."""
    schedule.every().day.at("19:00").do(job)
    print("üìÖ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ–º 19:00 –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∑–∞–¥–∞—á–∏...")

    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    job()
    # run_scheduler()
