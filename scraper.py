"""
books_scraper.scraper

–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö
—Å —Å–∞–π—Ç–∞ https://books.toscrape.com.

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–¥–Ω–æ–π –∫–Ω–∏–≥–µ (get_book_data);
- –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ (scrape_books);
- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è;
- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ artifacts/books_data.txt;
- –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (19:00).

–ê–≤—Ç–æ—Ä: Ekaterina Kuchina, –ú–§–¢–ò, 2025 –≥.
"""

import os
import time
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import schedule
from bs4 import BeautifulSoup

BASE_URL = "https://books.toscrape.com/"
OUTPUT_DIR = "artifacts"
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "books_data.txt")


def get_book_data(book_url: str) -> dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ —Å –µ—ë —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Parameters
    ----------
    book_url : str
        –ü–æ–ª–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏.

    Returns
    -------
    dict
        –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
        title, price, availability, rating, description,
        upc, product_type, price_excl_tax, price_incl_tax,
        tax, availability_count, number_of_reviews.

    Examples
    --------
    >>> get_book_data(
    ... "http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html"
    ... )
    {'title': 'A Light in the Attic', 'price': '¬£51.77', ...}
    """
    response = requests.get(book_url, timeout=15)
    response.encoding = "utf-8" if "utf" in response.apparent_encoding.lower() else "ISO-8859-1"
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "lxml")

    title = soup.find("div", class_="product_main").h1.get_text(strip=True)
    price = soup.find("p", class_="price_color").get_text(strip=True)
    availability = soup.find("p", class_="instock availability").get_text(strip=True)
    rating = soup.find("p", class_="star-rating")["class"][1]

    description_tag = soup.find("div", id="product_description")
    description = (
        description_tag.find_next_sibling("p").get_text(strip=True)
        if description_tag
        else ""
    )

    info_table = soup.find("table", class_="table table-striped")
    info = {
        row.th.get_text(strip=True): row.td.get_text(strip=True)
        for row in info_table.find_all("tr")
    }

    return {
        "title": title,
        "price": price,
        "availability": availability,
        "rating": rating,
        "description": description,
        "upc": info.get("UPC", ""),
        "product_type": info.get("Product Type", ""),
        "price_excl_tax": info.get("Price (excl. tax)", ""),
        "price_incl_tax": info.get("Price (incl. tax)", ""),
        "tax": info.get("Tax", ""),
        "availability_count": info.get("Availability", ""),
        "number_of_reviews": info.get("Number of reviews", ""),
    }


def scrape_books(is_save: bool = True, use_threads: bool = False) -> list[dict]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–Ω–∏–≥–∞—Ö —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ Books to Scrape.

    Parameters
    ----------
    is_save : bool, optional
        –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True).
    use_threads : bool, optional
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º).

    Returns
    -------
    list[dict]
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–∞—Ö.

    Examples
    --------
    >>> scrape_books(is_save=False, use_threads=True)
    [{'title': 'A Light in the Attic', 'price': '¬£51.77', ...}, ...]
    """
    all_books = []
    page = 1
    session = requests.Session()

    while True:
        page_url = (
            f"{BASE_URL}catalogue/page-{page}.html"
            if page > 1
            else f"{BASE_URL}index.html"
        )
        response = session.get(page_url, timeout=10)
        if response.status_code != 200:
            break

        soup = BeautifulSoup(response.text, "lxml")
        book_links = [urljoin(page_url, a["href"]) for a in soup.select("h3 > a")]
        if not book_links:
            break

        print(f"üìÑ Page {page} ‚Üí {len(book_links)} books")

        if use_threads:
            # –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = [executor.submit(get_book_data, link) for link in book_links]
                for f in as_completed(futures):
                    try:
                        all_books.append(f.result())
                    except Exception as exc:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ: {exc}")
        else:
            # –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            for link in book_links:
                try:
                    all_books.append(get_book_data(link))
                    time.sleep(0.05)
                except Exception as exc:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {exc}")

        next_button = soup.select_one("li.next > a")
        if not next_button:
            break
        page += 1

    if is_save:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        with open(OUTPUT_PATH, "w", encoding="utf-8") as file:
            for book in all_books:
                file.write(str(book) + "\n")
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_books)} –∫–Ω–∏–≥ –≤ {OUTPUT_PATH}")

    return all_books


def job() -> None:
    """
    –ï–¥–∏–Ω–∏—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–Ω–∏–≥.

    –ó–∞–ø—É—Å–∫–∞–µ—Ç scrape_books() –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª.
    """
    print("\nüïñ –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    scrape_books(is_save=True, use_threads=True)
    print("‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


def run_scheduler() -> None:
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –≤ 19:00.

    –§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –æ–∂–∏–¥–∞–Ω–∏—è
    –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Ä–∞–∑ –≤ –º–∏–Ω—É—Ç—É.
    """
    schedule.every().day.at("19:00").do(job)
    print("üìÖ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ–º 19:00 –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∑–∞–¥–∞—á–∏...")

    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
    job()
    # –î–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ:
    # run_scheduler()
